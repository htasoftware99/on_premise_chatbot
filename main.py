import os
import shutil
from typing import List, Optional
from datetime import datetime 
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv
from transformers import pipeline

# --- IMPORTLAR ---
from langchain_classic.memory import ConversationBufferMemory
from langchain_classic.chains import ConversationChain, RetrievalQA
from langchain_classic.prompts import PromptTemplate

# Community araÃ§larÄ±
from langchain_community.utilities import SerpAPIWrapper
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma

# Entegrasyonlar
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
# Text Splitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

import google.generativeai as genai

load_dotenv()

SERPAPI_KEY = os.getenv("SERPAPI_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OLLAMA_MODEL = "gemma3:4b"  
CHROMA_PATH = "./chroma_db"

# --- GEMINI YAPILANDIRMASI (STT Ä°Ã§in) ---
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("UYARI: GOOGLE_API_KEY bulunamadÄ±. Sesli sohbet Ã§alÄ±ÅŸmayabilir.")

# --- BAÅžLANGIÃ‡ AYARLARI ---
app = FastAPI()
os.environ["SERPAPI_API_KEY"] = SERPAPI_KEY

# --- PROMPTLAR ---
GENERAL_RAG_PROMPT = """
Sen uzman bir dÃ¶kÃ¼man asistanÄ±sÄ±n.
GÃ¶revin: AÅŸaÄŸÄ±da verilen "BaÄŸlam (Context)" iÃ§indeki bilgileri kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevaplamaktÄ±r.

Kurallar:
1. Sadece ve sadece verilen baÄŸlamdaki bilgilere gÃ¶re cevap ver.
2. DÃ¶kÃ¼man dÄ±ÅŸÄ±ndan bilgi uydurma.
3. EÄŸer sorunun cevabÄ± dÃ¶kÃ¼manda yoksa, net bir ÅŸekilde "Bu bilgi yÃ¼klenen dÃ¶kÃ¼manda yer almÄ±yor" de.
4. CevabÄ± verirken dÃ¶kÃ¼mandaki Ã¼sluba uygun, profesyonel ve aÃ§Ä±klayÄ±cÄ± ol.

BaÄŸlam (Context):
{context}

KullanÄ±cÄ± Sorusu: {question}

Cevap:
"""

SEARCH_PROMPT_TEMPLATE = """
Sen yardÄ±msever bir asistansÄ±n.
BugÃ¼nÃ¼n Tarihi: {current_date}

AÅŸaÄŸÄ±da Google arama sonuÃ§larÄ± verilmiÅŸtir. Bu bilgileri kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
DÄ°KKAT: Arama sonuÃ§larÄ±ndaki tarihleri kontrol et. EÄŸer etkinlik geÃ§miÅŸte kalmÄ±ÅŸsa o bilgiyi kullanma.
Sadece gelecekteki gÃ¼ncel etkinlikleri veya bilgileri ver.

KullanÄ±cÄ± Sorusu: {question}
Arama SonuÃ§larÄ±: {search_result}

Cevap:
"""

CHAT_PROMPT = """
Sen yardÄ±msever bir yapay zeka asistanÄ±sÄ±n.
KullanÄ±cÄ±ya nazik, kÄ±sa ve Ã¶z cevaplar ver.
BilmediÄŸin veya emin olmadÄ±ÄŸÄ±n konularda dÃ¼rÃ¼st ol.
"""

llm = OllamaLLM(model=OLLAMA_MODEL, temperature=0.3)

# Intent Detection (mDeBERTa Model)
classifier = pipeline("zero-shot-classification", model="MoritzLaurer/mDeBERTa-v3-base-mnli-xnli", device=-1)

def detect_intent(text: str) -> str:
    # Model Etiketleri
    labels = [
        "general conversation, greeting, self-introduction, or asking about the assistant's identity",
        "internet search regarding public figures, news, weather, prices, events, concerts, or objective facts",
        "question specific to the uploaded file or document analysis"
    ]
    
    result = classifier(text, labels)
    top_label = result['labels'][0]
    
    print(f"DEBUG (Model): AlgÄ±lanan Etiket -> '{top_label}'")

    if "internet search" in top_label:
        return "web_search_query"
    elif "uploaded file" in top_label:
        return "document_qa"
    else:
        return "general_chat"

memory = ConversationBufferMemory(return_messages=True)
search = SerpAPIWrapper()

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2", model_kwargs={'device': 'cpu'})
vector_store = None 

# --- MODELLER ---
class QueryRequest(BaseModel):
    query: str

class ResponseModel(BaseModel):
    response: str
    intent: str
    source: str

# --- ENDPOINTLER ---

# YENÄ° ENDPOINT: Ses dosyasÄ±nÄ± metne Ã§evir (Gemini ile)
@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    try:
        print(f"ðŸŽ¤ Ses dosyasÄ± alÄ±ndÄ±: {file.filename}")
        print(f"â„¹ï¸ Dosya Tipi: {file.content_type}")
        
        # DosyayÄ± belleÄŸe oku
        audio_bytes = await file.read()
        
        # Model TanÄ±mla
        print("ðŸ¤– gemini-2.5-flash modeli yÃ¼kleniyor...")
        model = genai.GenerativeModel("gemini-2.5-flash")
        
        # Modele gÃ¶nder
        print("ðŸ“¤ Ses verisi Google'a gÃ¶nderiliyor...")
        response = model.generate_content([
            "AÅŸaÄŸÄ±daki ses dosyasÄ±ndaki konuÅŸmalarÄ± birebir metne Ã§evir. Sadece duyduÄŸun kelimeleri yaz, baÅŸka hiÃ§bir aÃ§Ä±klama, yorum veya komut tekrarÄ± yapma.",
            {
                "mime_type": "audio/wav",
                "data": audio_bytes
            }
        ])
        
        print(f"âœ… Ã‡eviri BaÅŸarÄ±lÄ±: {response.text[:50]}...") # Ä°lk 50 karakteri logla
        return {"text": response.text.strip()}
    
    except Exception as e:
        # HATAYI BURADA GÃ–RECEÄžÄ°Z
        print(f"âŒ KRÄ°TÄ°K HATA: {e}")
        # HatanÄ±n detayÄ±nÄ± frontend'e de gÃ¶nderelim
        raise HTTPException(status_code=500, detail=f"Sunucu HatasÄ±: {str(e)}")

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    global vector_store
    try:
        # DosyayÄ± geÃ§ici olarak kaydet
        file_path = f"temp_{file.filename}"
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Dosya uzantÄ±sÄ±na gÃ¶re Loader seÃ§imi
        if file.filename.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        else:
            # .txt veya .md varsayÄ±yoruz
            loader = TextLoader(file_path, encoding="utf-8")
            
        documents = loader.load()
        
        # Chunklara bÃ¶l
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)
        
        # VektÃ¶r veritabanÄ±na gÃ¶m
        vector_store = Chroma.from_documents(
            documents=chunks, 
            embedding=embeddings, 
            persist_directory=CHROMA_PATH
        )
        
        os.remove(file_path) # Temizlik
        return {"message": f"{file.filename} baÅŸarÄ±yla analiz edildi."}
    except Exception as e:
        return {"error": str(e)} # Hata durumunda json dÃ¶nmeli

@app.post("/chat", response_model=ResponseModel)
async def chat_endpoint(request: QueryRequest):
    user_query = request.query.lower() # KÃ¼Ã§Ã¼k harfe Ã§evir
    
    # --- 1. AÅžAMA: KESÄ°N KURALLAR (RULE-BASED) ---
    # Modelin hata yapmasÄ±nÄ± engellemek iÃ§in bariz kelimeleri elle yakalÄ±yoruz.
    
    # A) Sohbet ZorlayÄ±cÄ±lar (Model search sanmasÄ±n diye)
    greeting_keywords = ["merhaba", "selam", "gÃ¼naydÄ±n", "iyi geceler", "kimsin", "adÄ±n ne", "sen kimsin", "nasÄ±lsÄ±n", "naber", "benim adÄ±m", "ben hasan"]
    
    # B) Arama ZorlayÄ±cÄ±lar (Model chat sanmasÄ±n diye) -> BURASI YENÄ°
    search_keywords = ["konser", "bilet", "maÃ§", "etkinlik", "hava durumu", "dolar", "euro", "altÄ±n", "kaÃ§ tl", "kaÃ§ para", "fiyatÄ±", "nerede", "ne zaman", "kimdir", "nedir"]

    intent = ""

    if any(k in user_query for k in greeting_keywords):
        intent = "general_chat"
        print(f"DEBUG (Rule): Keyword ile CHAT seÃ§ildi.")
        
    elif any(k in user_query for k in search_keywords):
        intent = "web_search_query"
        print(f"DEBUG (Rule): Keyword ile SEARCH seÃ§ildi.")
        
    else:
        # --- 2. AÅžAMA: YAPAY ZEKA KARARI ---
        # Kelime listelerinde yoksa modele sor (Ã–rn: "KarnÄ±m aÄŸrÄ±yor", "Åžiir yaz")
        intent = detect_intent(request.query)
    
    # --- 3. AÅžAMA: DOSYA KONTROLÃœ ---
    if intent == "document_qa" and vector_store is None:
        intent = "general_chat"

    response_text = ""
    source_used = ""

    # --- 4. AÅžAMA: Ä°ÅžLEM ---
    today = datetime.now().strftime("%Y-%m-%d")
    current_year = datetime.now().year

    if intent == "web_search_query":
        print(f"--> Intent: Web Search ({request.query})")
        try:
            enriched_query = f"{request.query} {current_year}"
            
            print(f"   (Google Query: {enriched_query})")
            
            search_result = search.run(enriched_query)
            
            if not search_result or len(search_result) < 5:
                search_result = search.run(request.query)

            final_prompt = SEARCH_PROMPT_TEMPLATE.format(
                current_date=today, 
                question=request.query, 
                search_result=search_result
            )
            
            response_text = llm.invoke(final_prompt)
            source_used = "SerpApi (Google)"
            
        except Exception as e:
            print(f"Search HatasÄ±: {e}")
            response_text = llm.invoke(request.query)
            intent = "general_chat"
            source_used = "LLM (Search BaÅŸarÄ±sÄ±z)"

    elif intent == "document_qa" and vector_store is not None:
        print(f"--> Intent: RAG ({request.query})")
        retriever = vector_store.as_retriever()
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm, 
            chain_type="stuff", 
            retriever=retriever,
            chain_type_kwargs={"prompt": PromptTemplate(
                template=GENERAL_RAG_PROMPT,
                input_variables=["context", "question"]
            )}
        )
        response_text = qa_chain.run(request.query)
        source_used = "YÃ¼klenen DÃ¶kÃ¼man (RAG)"
        
    else:
        print(f"--> Intent: Chat ({request.query})")
        
        template = f"""{CHAT_PROMPT}
        
        GeÃ§miÅŸ Sohbet:
        {{history}}
        
        KullanÄ±cÄ±: {{input}}
        Asistan:"""
        
        PROMPT = PromptTemplate(input_variables=["history", "input"], template=template)
        
        conversation = ConversationChain(
            prompt=PROMPT,
            llm=llm, 
            memory=memory
        )
        response_text = conversation.predict(input=request.query)
        source_used = "LLM + HafÄ±za"

    return {
        "response": response_text,
        "intent": intent,
        "source": source_used
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)